# Final-Year-Project
All scripts and notebooks used to implement machine learning model explainability using LIME for models trained to predict brain tumours in Magnetic-Resonance Imaging

The aim of this project is to evaluate and test explainability methods and hopefully provide a means to opening a ‘black-box’ machine learning model which has been trained to classify tumours in MRI brain scans. Explainability isn’t a mature area of research and because of this there aren’t any universally accepted tools to provide explainability. That being said, there is currently a lot of effort being channelled into it, and as a result many new explainability methods are being tested. This project will focus on recent method called Local Interpretable Model-Agnostic Explainability, or LIME for short, which will be applied to one or more machine learning models called Convolutional Neural Networks. By training these networks and applying LIME to them, hopefully I will be able to peer into the black-box and gain and understanding into what influences the models decisions, and evaluate whether the models can be trusted, or if they are inherently flawed by some learned bias in the data.


All references are contained in the Final Year Report PDF File
